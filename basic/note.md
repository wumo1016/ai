## 提示词

- User Prompt(用户提示词)
  - 即用户输入的文本
  - 例如："你好"、"请介绍一下你自己" 等
- System Prompt(系统提示词)
  - 用于描述 AI 模型的角色、性格、背景知识、语气等等(AI 内置的用户提示词, 用于引导模型生成符合预期的输出, 和用户提示词一起发送给 AI)
  - 例如："你是一个专业的翻译"、"请用中文回答" 等

## 分词器

- 将文本转换为 token 的工具(按顺序切分成一个个的 token)
  - 例如：将 "Hello, world!" 转换为 ["Hello", ",", "world", "!"]

## token

- 也称为词元
- 是大模型理解内容的最小单元
- 每个 token 都有一个唯一的编号, 称为 Token ID
  - 例如：["Hello", ",", "world", "!"] 对应的 Token ID 为 [1, 2, 3, 4]

## 大语言模型 LLM

- 例如:
  - chatgpt
  - deepseek
  - gemini
  - claude
  - 豆包
- 大模型的任务就是计算在给定 token 序列后, 应当续写哪些 token
- 计算的过程:

  - 大模型会一个 token 一个 token 的算
  - 每次计算会把新生成的 token 添加到原有入的 token 序列中
  - 再投入模型中, 算出下一个 token, 再加入 token 序列中, 如此往复, 直到生成结束
  - 所以本质是在不断续写 token 串

- 自回归(根据前面的 token 预测下一个 token)
- 语义空间(例如：所有汉字组合的含义)

## RAG (Retrieval-Augmented Generation 检索增强生成)

- RAG 是一种结合了信息检索 (Retrieval) 和生成式模型 (Generation) 的技术方案。
- 工作流程:
  - 首先根据用户输入, 从知识库或文档库中检索出相关内容片段(如段落、句子)
  - 然后将这些检索到的内容与原始输入一同送入大语言模型, 由模型进行综合并生成最终的答案或文本
- 优势:
  - 能让模型“看见”更多实时或扩展知识, 减轻模型“遗忘”或不懂冷门知识的缺陷
  - 实现个性化、定制化结果(如结合个人笔记、企业文档)。
- 典型应用:
  - 企业知识问答、智能客服、法律/医疗/教育等领域。

## Transformer 架构

- 来自谷歌的论文《Attention is all you need》
- 大模型使用的核心架构
- 采用了自注意力机制(很好的捕捉上下文关系)
- 参数
  - 单位: 十亿(Billion)
- 稠密模型(Dense Model)

  - 每次计算都调用所有的参数

- 稀疏模型(Sparse Model)

  - 每次计算只会调用相关的模型
    - 通过门控网络(gating network), 给每个问题分配合适的"专家"
  - 例如
    - 混合专家模型(MoE)(Mixture of Experts)(例如: deepseek、Gork)

## 预训练(Pre-training)

- 为了打造一个基座模型
- 通过"反向传播"的方法自行调整参数

  - 正向传播: 输入问题, 输出结果
  - 反向传播
    - 输入问题, 输出结果, 使其与正确结果进行对比, 看看差了多少, 这一步就是计算损失(loss)
    - 然后, 误差会从输出层反向传递到每一层, 通过链式法则计算每个参数对误差的影响(梯度)
    - 最后, 使用这些梯度来调整神经网络的参数, 使下一次预测更接近真实结果

- 因为工作量巨大, 所以使用"自监督学习"

## 后训练(Post-training)

- 因为经过预训练的基座模型并不能直接使用, 所以需要"微调"
- 微调
  - 监督微调(SFT)
    - 给 AI 提供带标注的数据集(人工数据标注员)
- 强化学习
  - GRPO: 不断告诉 AI 哪些答案是对的
  - RLHF
    - 人工数据标注员: 将回答按顺序排序
    - 奖励模型(Rewrad Model): 答案太多, 所以直接使用奖励模型对 AI 的回答进行打分, 这种叫做(RLHF)基于人工反馈的强化学习

## COT

- 思维链

## 蒸馏模型

- 满血版的高仿版
- 蒸馏模型(Distillation Model)是一种模型压缩的方法, 主要目的是将一个庞大且性能很强的“教师模型”(Teacher)中的知识, 迁移到一个体积较小、计算资源消耗更低的“学生模型”(Student)。训练过程中, 学生模型不仅学习教师模型对数据的标准结果(标签), 还学习教师模型对每个样本预测的“软输出”(概率分布), 这样学生模型能在保留高性能的同时, 大幅减少参数量和推理算力需求。
- 主要流程：
  - 先训练出表现很好的大模型作为“教师”
  - 用大模型对样本进行推理, 得到软标签(如类别的概率分布)
  - 小模型(学生)在训练时, 不只是学硬标签, 还学习这些软标签
- 优点：
  - 大幅减少模型体积, 提高推理速度
  - 在保证一定性能的前提下, 可以在边缘设备等算力受限环境部署
- 缺点：
  - 学生模型能力通常不可能完全追平教师模型

## 量化模型

- 满血版的压缩版

- 量化模型(Quantized Model)是一种通过将模型中的参数和计算从高精度(如 32 位浮点数)压缩为低精度(如 8 位整数)实现模型压缩和加速的方法。这样可以显著减少模型的存储体积和推理时所需的计算资源。
  - 优点：
    - 大幅减少模型大小, 降低存储和内存占用
    - 显著提升推理速度, 降低功耗, 适合在移动端和嵌入式设备部署
  - 缺点：
    - 量化后可能略微损失模型的推理精度, 尤其在极端压缩时
  - 应用场景：
    - 算力受限或需要高速响应的设备, 如手机、IoT 设备、浏览器等

## AI Agent

- 即一个智能体(Agent), 它可以根据用户输入, 调用不同的模型, 完成不同的任务
- 负责用户、工具(Agent Tool)、模型之间的交互
- 缺点
  - 虽然告诉了它要返回什么格式的结果, 但它毕竟是一个概率模型, 还是可能返回格式不对(如果发现返回的结果不符合要求, 会尝试重新请求)

## Agent Tool

- 即一个工具(Tool), 它可以帮助 Agent 完成特定的任务
